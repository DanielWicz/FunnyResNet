{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleResNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EI9kVD6HJCo",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial for building own layer in the TensorFlow2\n",
        "# Made by Daniel Wiczew\n",
        "# Cotact: daniel.wiczew@gmail.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAA7W6edefUy",
        "colab_type": "code",
        "outputId": "9f5e01a1-e113-4f4e-b20d-478e3578c029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        }
      },
      "source": [
        "# Please download TensorFlow 2.0 or higher to work with the tutorial\n",
        "!pip uninstall --yes tensorflow\n",
        "!pip install tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (46.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-2.1.1 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxKCXMaoekS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are the libraries needed for the tutorial\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import imshow\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_IBoy6lTqqz",
        "colab_type": "code",
        "outputId": "3df07223-9455-4452-db56-f9aac60f3225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        }
      },
      "source": [
        "# Make sure that the GPU is available and the version is 2.0 or higher\n",
        "print(tf.__version__)\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "tf.test.is_gpu_available()  #>>> True"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 4521178795135527432\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 4546857645181843592\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 11993612135867426182\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15956161332\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 13366759605019209822\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n",
            "WARNING:tensorflow:From <ipython-input-2-4694b2fd5e85>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEJUnrM6nXaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and load CIFAR10 dataset to the memory.\n",
        "# If you don't have enough memory (which is not the case on the google colab), then you have to download it manually\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA1mxQuan4Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize images\n",
        "x_train = x_train * (1./255.)\n",
        "x_test = x_test * (1./255.)\n",
        "\n",
        "# Standarize Images\n",
        "mean = np.mean(x_train,axis=(0, 1, 2, 3))\n",
        "std = np.std(x_train,axis=(0, 1, 2, 3))\n",
        "x_train = (x_train-mean)/(std + 1e-7)\n",
        "x_test = (x_test-mean)/(std + 1e-7)\n",
        "\n",
        "# Turn classes to one-hot-encoding matrices\n",
        "OUTPUT_CLASSES = y_train.max() + 1\n",
        "y_train = tf.keras.utils.to_categorical(y_train, OUTPUT_CLASSES)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, OUTPUT_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C23HNqT-pbCU",
        "colab_type": "code",
        "outputId": "b80b5566-67e4-4a15-e741-38fbb6ea31c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "# Example image\n",
        "imshow(x_test[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f48591ccf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYOgecuApwCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic constants\n",
        "# 300 should be enough to train the model\n",
        "EPOCHS = 300\n",
        "BATCH_SIZE = 256\n",
        "INPUT_SHAPE = x_train[255].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-QU0WMZxuYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Augumentation\n",
        "# You can play with the parameters to get better accuracy\n",
        "imagegen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=45, \n",
        "                                                           width_shift_range=0.25,\n",
        "                                                           height_shift_range=0.25,\n",
        "                                                           horizontal_flip=True,\n",
        "                                                           zoom_range=0.25,\n",
        "                                                           shear_range=0.15)\n",
        "imagegen.fit(x_train)\n",
        "dataflow = imagegen.flow(x_train, y_train, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiupdUjbfNzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here is the first resnet layer, that does preserve input shape\n",
        "# ResNET Layer\n",
        "class ResidualLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, f=None, fillter_size_top=None,\n",
        "               fillter_size_mid=None, fillter_size_bot=None):\n",
        "    super(ResidualLayer, self).__init__()\n",
        "    self.conv_top_1 = tf.keras.layers.Conv2D(fillter_size_top, (1, 1),\n",
        "                                             strides=(1, 1), padding='valid')\n",
        "    # Divided by 2 to ensure different parameters\n",
        "    self.conv_top_2 = tf.keras.layers.Conv2D(fillter_size_top//4, (1, 1),\n",
        "                                             strides=(1, 1), padding='valid')\n",
        "    self.conv_mid_1 = tf.keras.layers.Conv2D(fillter_size_mid, (f, f),\n",
        "                                             strides=(1, 1), padding='same')\n",
        "    # Divided by 2 to ensure different parameters\n",
        "    self.conv_mid_2 = tf.keras.layers.Conv2D(fillter_size_mid//4, (f, f),\n",
        "                                             strides=(1, 1), padding='same')\n",
        "    self.conv_bot_1 = tf.keras.layers.Conv2D(fillter_size_bot, (1, 1),\n",
        "                                             strides=(1, 1), padding='valid')\n",
        "    # The outputs have to be the same to add up\n",
        "    self.conv_bot_2 = tf.keras.layers.Conv2D(fillter_size_bot, (1, 1),\n",
        "                                             strides=(1, 1), padding='valid')\n",
        "    \n",
        "    self.batch_norm_top_1 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.batch_norm_top_2 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.batch_norm_mid_1 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.batch_norm_mid_2 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.batch_norm_bot_1 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.batch_norm_bot_2 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "\n",
        "    self.activation_relu = tf.keras.layers.Activation('relu')\n",
        "    self.add_op = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, input_x, training=False):\n",
        "    x_shortcut = input_x\n",
        "\n",
        "    ##PATH 1\n",
        "    x_path_1 = input_x\n",
        "    # First CONV block of path 1\n",
        "    x_path_1 = self.conv_top_1(x_path_1)\n",
        "    x_path_1 = self.batch_norm_top_1(x_path_1, training=training)\n",
        "    x_path_1 = self.activation_relu(x_path_1)\n",
        "\n",
        "    # Second CONV block of path 1\n",
        "    x_path_1 = self.conv_mid_1(x_path_1)\n",
        "    x_path_1 = self.batch_norm_mid_1(x_path_1, training=training)\n",
        "    x_path_1 = self.activation_relu(x_path_1)\n",
        "\n",
        "    # Third CONV block of path 1\n",
        "    x_path_1 = self.conv_bot_1(x_path_1)\n",
        "    x_path_1 = self.batch_norm_bot_1(x_path_1, training=training)\n",
        "\n",
        "    ##PATH 2\n",
        "    x_path_2 = input_x\n",
        "    # First CONV block of path 1\n",
        "    x_path_2 = self.conv_top_2(x_path_2)\n",
        "    x_path_2 = self.batch_norm_top_2(x_path_2, training=training)\n",
        "    x_path_2 = self.activation_relu(x_path_2)\n",
        "\n",
        "    # Second CONV block of path 1\n",
        "    x_path_2 = self.conv_mid_2(x_path_2)\n",
        "    x_path_2 = self.batch_norm_mid_2(x_path_2, training=training)\n",
        "    x_path_2 = self.activation_relu(x_path_2)\n",
        "\n",
        "    # Third CONV block of path 1\n",
        "    x_path_2 = self.conv_bot_2(x_path_2)\n",
        "    x_path_2 = self.batch_norm_bot_2(x_path_2, training=training)\n",
        "\n",
        "    # Addition of PATH 1 and PATH 2\n",
        "    x = self.add_op([x_path_1, x_path_2])\n",
        "    # Addition to the shortcut path\n",
        "    x = self.add_op([x, x_shortcut])\n",
        "    x_output = self.activation_relu(x)\n",
        "\n",
        "    return x_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcextqIwL5i_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here is the second resnet layer, that does not preserve input shape\n",
        "# ResNET layer\n",
        "class ResidualLayerScal(tf.keras.layers.Layer):\n",
        "  def __init__(self, f=None, s=None, fillter_size_top=None,\n",
        "               fillter_size_mid=None, fillter_size_bot=None):\n",
        "    super(ResidualLayerScal, self).__init__()\n",
        "    self.conv_top_1 = tf.keras.layers.Conv2D(fillter_size_top, (1, 1),\n",
        "                                             strides=(1, 1), padding='valid')\n",
        "    # Make the hyperparameters different \n",
        "    self.conv_top_2 = tf.keras.layers.Conv2D(fillter_size_top//4, (1, 1),\n",
        "                                             strides=(1, 1), padding='valid')\n",
        "    self.conv_mid_1 = tf.keras.layers.Conv2D(fillter_size_mid, (f, f),\n",
        "                                             strides=(1, 1), padding='same')\n",
        "    # Make the hyperparameters different \n",
        "    self.conv_mid_2 = tf.keras.layers.Conv2D(fillter_size_mid//4, (f, f),\n",
        "                                             strides=(1, 1), padding='same')\n",
        "    self.conv_bot_1 = tf.keras.layers.Conv2D(fillter_size_bot, (1, 1),\n",
        "                                             strides=(s, s), padding='valid')\n",
        "    # You can't make the hyperparameters different, the output have to be the same\n",
        "    self.conv_bot_2 = tf.keras.layers.Conv2D(fillter_size_bot, (1, 1),\n",
        "                                             strides=(s, s), padding='valid')\n",
        "    self.conv_scal = tf.keras.layers.Conv2D(fillter_size_bot, (1, 1),\n",
        "                                             strides=(s, s), padding='valid')\n",
        "    \n",
        "    self.batch_norm_top_1 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.batch_norm_top_2 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.batch_norm_mid_1 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.batch_norm_mid_2 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.batch_norm_bot_1 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.batch_norm_bot_2 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.batch_norm_scal = tf.keras.layers.BatchNormalization(axis=3)\n",
        "\n",
        "    self.activation_relu = tf.keras.layers.Activation('relu')\n",
        "    self.add_op = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, input_x, training=False):\n",
        "    x_shortcut = input_x\n",
        "\n",
        "    ##PATH 1\n",
        "    x_path_1 = input_x\n",
        "    # First CONV block of path 1\n",
        "    x_path_1 = self.conv_top_1(x_path_1)\n",
        "    x_path_1 = self.batch_norm_top_1(x_path_1, training=training)\n",
        "    x_path_1 = self.activation_relu(x_path_1)\n",
        "\n",
        "    # Second CONV block of path 1\n",
        "    x_path_1 = self.conv_mid_1(x_path_1)\n",
        "    x_path_1 = self.batch_norm_mid_1(x_path_1, training=training)\n",
        "    x_path_1 = self.activation_relu(x_path_1)\n",
        "\n",
        "    # Third CONV block of path 1\n",
        "    x_path_1 = self.conv_bot_1(x_path_1)\n",
        "    x_path_1 = self.batch_norm_bot_1(x_path_1, training=training)\n",
        "\n",
        "    ##PATH 2\n",
        "    x_path_2 = input_x\n",
        "    # First CONV block of path 1\n",
        "    x_path_2 = self.conv_top_2(x_path_2)\n",
        "    x_path_2 = self.batch_norm_top_2(x_path_2, training=training)\n",
        "    x_path_2 = self.activation_relu(x_path_2)\n",
        "\n",
        "    # Second CONV block of path 1\n",
        "    x_path_2 = self.conv_mid_2(x_path_2)\n",
        "    x_path_2 = self.batch_norm_mid_2(x_path_2, training=training)\n",
        "    x_path_2 = self.activation_relu(x_path_2)\n",
        "\n",
        "    # Third CONV block of path 1\n",
        "    x_path_2 = self.conv_bot_2(x_path_2)\n",
        "    x_path_2 = self.batch_norm_bot_2(x_path_2, training=training)\n",
        "\n",
        "    # Addition of PATH 1 and PATH 2\n",
        "    x = self.add_op([x_path_1, x_path_2])\n",
        "    # Scaling Block\n",
        "    x_shortcut = self.conv_scal(x_shortcut)\n",
        "    x_shortcut = self.batch_norm_scal(x_shortcut, training=training)\n",
        "    # Addition to the shortcut path\n",
        "    x = self.add_op([x, x_shortcut])\n",
        "    x_output = self.activation_relu(x)\n",
        "\n",
        "    return x_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Dx48qELQGGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Then the layers are combined into a model\n",
        "class FunnyResNet(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(FunnyResNet, self).__init__()\n",
        "    self.batchnorm_lay1 = tf.keras.layers.BatchNormalization(axis=3)\n",
        "    self.conv_lay1 = tf.keras.layers.Conv2D(64, strides=(2, 2), kernel_size=(3, 3), activation='relu')\n",
        "    self.act_lay1 = tf.keras.layers.Activation('relu')\n",
        "\n",
        "\n",
        "  # Our ResNet block 1\n",
        "    self.resnet_b1_lay1 = ResidualLayerScal(f=3, s=2, fillter_size_top=128,\n",
        "                                            fillter_size_mid=128, fillter_size_bot=256)\n",
        "    self.resnet_b1_lay2 = ResidualLayer(f=3, fillter_size_top=128,\n",
        "                                        fillter_size_mid=128, fillter_size_bot=256)\n",
        "    self.resnet_b1_lay3 = ResidualLayer(f=3, fillter_size_top=128,\n",
        "                                        fillter_size_mid=128, fillter_size_bot=256)\n",
        "    self.resnet_b1_lay4 = ResidualLayer(f=3, fillter_size_top=128,\n",
        "                                        fillter_size_mid=128, fillter_size_bot=256)\n",
        "    self.resnet_b1_lay5 = ResidualLayer(f=3, fillter_size_top=128,\n",
        "                                        fillter_size_mid=128, fillter_size_bot=256)\n",
        "\n",
        "    # Our ResNet block 2\n",
        "    self.resnet_b2_lay1 = ResidualLayerScal(f=5, s=2, fillter_size_top=256,\n",
        "                                            fillter_size_mid=256, fillter_size_bot=512)\n",
        "    self.resnet_b2_lay2 = ResidualLayer(f=5, fillter_size_top=256,\n",
        "                                        fillter_size_mid=256, fillter_size_bot=512)\n",
        "    self.resnet_b2_lay3 = ResidualLayer(f=5, fillter_size_top=256,\n",
        "                                        fillter_size_mid=256, fillter_size_bot=512)\n",
        "    self.resnet_b2_lay4 = ResidualLayer(f=5, fillter_size_top=256,\n",
        "                                        fillter_size_mid=256, fillter_size_bot=512)\n",
        "    self.resnet_b2_lay5 = ResidualLayer(f=5, fillter_size_top=256,\n",
        "                                        fillter_size_mid=256, fillter_size_bot=512)\n",
        "\n",
        "  # Our ResNet block 3\n",
        "    self.resnet_b3_lay1 = ResidualLayerScal(f=7, s=2, fillter_size_top=512,\n",
        "                                            fillter_size_mid=512, fillter_size_bot=1024)\n",
        "    self.resnet_b3_lay2 = ResidualLayer(f=7, fillter_size_top=512,\n",
        "                                        fillter_size_mid=512, fillter_size_bot=1024)\n",
        "    self.resnet_b3_lay3 = ResidualLayer(f=7, fillter_size_top=512,\n",
        "                                        fillter_size_mid=512, fillter_size_bot=1024)\n",
        "    self.resnet_b3_lay4 = ResidualLayer(f=7, fillter_size_top=512,\n",
        "                                        fillter_size_mid=512, fillter_size_bot=1024)\n",
        "    self.resnet_b3_lay5 = ResidualLayer(f=7, fillter_size_top=512,\n",
        "                                        fillter_size_mid=512, fillter_size_bot=1024)\n",
        "\n",
        "\n",
        "    self.flat_lay = tf.keras.layers.Flatten()\n",
        "    self.dense_lay2 = tf.keras.layers.Dense(OUTPUT_CLASSES, activation='softmax')\n",
        "\n",
        "  def call(self, x_input, training=False):\n",
        "    x = self.conv_lay1(x_input)\n",
        "    x = self.batchnorm_lay1(x, training=training)\n",
        "    x = self.act_lay1(x)\n",
        "    # Residual block 1\n",
        "    x = self.resnet_b1_lay1(x, training=training)\n",
        "    x = self.resnet_b1_lay2(x, training=training)\n",
        "    x = self.resnet_b1_lay3(x, training=training)\n",
        "    x = self.resnet_b1_lay4(x, training=training)\n",
        "    x = self.resnet_b1_lay5(x, training=training)\n",
        "    # Residual block 2\n",
        "    x = self.resnet_b2_lay1(x, training=training)\n",
        "    x = self.resnet_b2_lay2(x, training=training)\n",
        "    x = self.resnet_b2_lay3(x, training=training)\n",
        "    x = self.resnet_b2_lay4(x, training=training)\n",
        "    x = self.resnet_b2_lay5(x, training=training)\n",
        "    # Residual block 3\n",
        "    x = self.resnet_b3_lay1(x, training=training)\n",
        "    x = self.resnet_b3_lay2(x, training=training)\n",
        "    x = self.resnet_b3_lay3(x, training=training)\n",
        "    x = self.resnet_b3_lay4(x, training=training)\n",
        "    x = self.resnet_b3_lay5(x, training=training)\n",
        "\n",
        "    x = self.flat_lay(x)\n",
        "    x = self.dense_lay2(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X10SmzJ0p9ZM",
        "colab_type": "code",
        "outputId": "a868e36f-e801-4c51-beaf-f394c6f2d607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "model = FunnyResNet()\n",
        "# Set initial weight shapes based on the initial input\n",
        "model(x_train[0:2])\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.02)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer funny_res_net_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddyb4lJzvg3v",
        "colab_type": "code",
        "outputId": "78289253-b7a0-4203-dd86-f96912459e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(dataflow, epochs=EPOCHS, shuffle=True,\n",
        "                    validation_data=[x_test, y_test])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 196 steps, validate on 10000 samples\n",
            "Epoch 1/300\n",
            "196/196 [==============================] - 69s 352ms/step - loss: 5.3244 - accuracy: 0.1312 - val_loss: 20.6620 - val_accuracy: 0.1532\n",
            "Epoch 2/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 2.0875 - accuracy: 0.2100 - val_loss: 2.1129 - val_accuracy: 0.2324\n",
            "Epoch 3/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.9589 - accuracy: 0.2592 - val_loss: 1.8601 - val_accuracy: 0.2832\n",
            "Epoch 4/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.8726 - accuracy: 0.2965 - val_loss: 1.7714 - val_accuracy: 0.3312\n",
            "Epoch 5/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.8119 - accuracy: 0.3248 - val_loss: 1.6825 - val_accuracy: 0.3661\n",
            "Epoch 6/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.7564 - accuracy: 0.3464 - val_loss: 1.6841 - val_accuracy: 0.3654\n",
            "Epoch 7/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.7058 - accuracy: 0.3647 - val_loss: 1.5543 - val_accuracy: 0.4275\n",
            "Epoch 8/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.6470 - accuracy: 0.3932 - val_loss: 1.5148 - val_accuracy: 0.4375\n",
            "Epoch 9/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.6062 - accuracy: 0.4072 - val_loss: 1.5296 - val_accuracy: 0.4496\n",
            "Epoch 10/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.5578 - accuracy: 0.4267 - val_loss: 1.3897 - val_accuracy: 0.4846\n",
            "Epoch 11/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.5266 - accuracy: 0.4388 - val_loss: 1.3826 - val_accuracy: 0.4917\n",
            "Epoch 12/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.4850 - accuracy: 0.4568 - val_loss: 1.5102 - val_accuracy: 0.4787\n",
            "Epoch 13/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.4477 - accuracy: 0.4715 - val_loss: 1.3444 - val_accuracy: 0.5197\n",
            "Epoch 14/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.4123 - accuracy: 0.4869 - val_loss: 1.3167 - val_accuracy: 0.5244\n",
            "Epoch 15/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.3772 - accuracy: 0.5015 - val_loss: 1.2371 - val_accuracy: 0.5534\n",
            "Epoch 16/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.3477 - accuracy: 0.5147 - val_loss: 1.3482 - val_accuracy: 0.5435\n",
            "Epoch 17/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.3119 - accuracy: 0.5261 - val_loss: 1.2771 - val_accuracy: 0.5586\n",
            "Epoch 18/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.2852 - accuracy: 0.5385 - val_loss: 1.2269 - val_accuracy: 0.5691\n",
            "Epoch 19/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.2505 - accuracy: 0.5534 - val_loss: 1.2160 - val_accuracy: 0.5831\n",
            "Epoch 20/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.2164 - accuracy: 0.5668 - val_loss: 1.3800 - val_accuracy: 0.5400\n",
            "Epoch 21/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.2118 - accuracy: 0.5688 - val_loss: 1.2469 - val_accuracy: 0.5729\n",
            "Epoch 22/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.1795 - accuracy: 0.5845 - val_loss: 1.4144 - val_accuracy: 0.5748\n",
            "Epoch 23/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.1287 - accuracy: 0.6024 - val_loss: 1.0353 - val_accuracy: 0.6508\n",
            "Epoch 24/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.0823 - accuracy: 0.6196 - val_loss: 1.1608 - val_accuracy: 0.6019\n",
            "Epoch 25/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.2892 - accuracy: 0.5429 - val_loss: 1.2631 - val_accuracy: 0.5420\n",
            "Epoch 26/300\n",
            "196/196 [==============================] - 59s 301ms/step - loss: 1.2182 - accuracy: 0.5653 - val_loss: 1.2869 - val_accuracy: 0.5544\n",
            "Epoch 27/300\n",
            " 43/196 [=====>........................] - ETA: 43s - loss: 1.1309 - accuracy: 0.5986"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}